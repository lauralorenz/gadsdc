### Before

Read [Generalized Linear Models (GLM)](http://www.wright.edu/~thaddeus.tarpey/ES714glm.pdf), focusing on section one, _Logistic Regression_.

Optional:

 * Check out this [deck](http://www.mc.vanderbilt.edu/gcrc/workshop_files/2004-11-12.pdf) introducing logistic regression.


### Questions

 * Say you run an L1-regularized regression and it selects five features (all the other coefficients are zero). You could fit an OLS regression with these five features. How would the the two techniques compare, in terms of computation and results?
 * Consider thinking of boolean multinomial Naive Bayes likelihood probabilities as coefficients on word dummy features. How are they similar or different as compared with linear regression coefficients? Could we use a linear model for probability labels?
 * What other thoughts, comments, concerns, and questions do you have? What's on your mind?


### During

Application presentation.

Question review.

Bring in standard GA logistic assignment from CA here?

[Logistic Regression w/ Statsmodel - Well Switching in Bangledesh](http://nbviewer.ipython.org/github/carljv/Will_it_Python/blob/master/ARM/ch5/arsenic_wells_switching.ipynb)

 * [Fast Logistic Regression: Mahout](https://cwiki.apache.org/MAHOUT/logistic-regression.html)
 * [Fast Logistic Regression: Vowpal Wabbit](https://github.com/JohnLangford/vowpal_wabbit/wiki)
 * [Fast Logistic Regression: LIBLINEAR](http://www.csie.ntu.edu.tw/~cjlin/liblinear/)

### After

[Naive Bayes v. Logistic Regression](http://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf)

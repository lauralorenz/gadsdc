# Logistic Regression

### `grad` example

This dataset is [from](http://www.ats.ucla.edu/stat/r/dae/logit.htm) UCLA's Institute for Digital Research and Education, which is a great resource for statistical techniques and software generally. The data is meant to represent whether a student was admitted into a graduate program (`admit`) based on GRE score (`gre`), undergraduate GPA (`gpa`), and the "rank" of their undergraduate school (`rank`), where `1` is the most prestigious and `4` is the least.

```{r}
grad <- read.csv("http://www.ats.ucla.edu/stat/data/binary.csv")
head(grad)
```

We can explore trends graphically to see if the data behaves as we expect.

```{r}
boxplot(gre ~ admit, data=grad, xlab="GRE score", ylab="not/admitted", horizontal=TRUE)
boxplot(gpa ~ admit, data=grad, xlab="GPA", ylab="not/admitted", horizontal=TRUE)
barplot(prop.table(table(grad$admit, grad$rank), 2), xlab="school rank", ylab="percent not/admitted")
```

Note that the apparent relationship to school rank is not linear. The values are numeric, but it isn't really a continuous variable. We want to treat the school `rank` as categorical.

```{r}
grad$rank <- as.factor(grad$rank)
```

Now we can fit a logistic regression model to the data.

```{r}
model <- glm(admit ~ ., data=grad, family="binomial")
summary(model)
```

Note that the coefficients come out as we expected.

Deviance is -2 times the log likelihood, and the residuals are the point-wise contributions to this. Comparing the null and residual deviance gives you some idea of how well the model fits the training data. Fisher Scoring iterations refer to the iterative maximum likelihood algorithm that is used to fit the model.

Let's predict for the training data to see how it works for logistic models. We can get log odds or probabilities out.

```{r}
logodds <- predict(model, grad)
probs <- predict(model, grad, type="response")
head(logodds)
head(probs)
head(log(probs/(1-probs)))
head(exp(logodds)/(1+exp(logodds)))
```

We're interested in how our predictions compare to reality. Usually we'll look at probabilities.

```{r}
head(data.frame(actual=grad$admit, prediction=probs))
```

You may need to set a cut-off, but we can evaluate performance in several ways already. The `ROCR` package is one option. The `pROC` package is another specifically for ROC curves.

```{r, tidy=FALSE}
suppressPackageStartupMessages(library('ROCR'))
# ROCR works on it's own `prediction` objects, which must be built like this:
pred <- prediction(predictions=probs, labels=grad$admit)
# Now the `performance` function will help us
# Accuracy: what percentage are correctly classified?
acc <- performance(pred, measure='acc')
plot(acc)
# Precision: what percentage of positive predictions are correct?
prec <- performance(pred, measure='prec')
plot(prec)
# Recall: what percentage of all positives were identified?
rec <- performance(pred, measure='rec')
plot(rec)
# Receiver Operator Curve (ROC)
roc <- performance(pred, 'tpr', 'fpr')
plot(roc)
# Here's one way to get the AUC (Area Under the roC) itself
auc <- performance(pred, measure='auc')
auc@y.values[[1]]
```
